{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configure folder structure and cuda enabling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if not os.path.exists(\"/kaggle/working/images\"):\n",
    "    os.mkdir('/kaggle/working/images')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Label Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 16\n",
    "shuffle=False\n",
    "num_classes = len(os.listdir('/kaggle/input/aid-scene-classification-datasets/AID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotAID(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, num_classes):\n",
    "        self.dataset = base_dataset\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        one_hot = F.one_hot(torch.tensor(label), num_classes=self.num_classes)\n",
    "        return img,one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('/kaggle/input/aid-scene-classification-datasets/AID', transform=transform)\n",
    "sample_dataset = RandomSampler(dataset, num_samples=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dataset = OneHotAID(dataset, num_classes)\n",
    "data_loader = DataLoader(one_hot_dataset, batch_size=BATCH, shuffle=shuffle, num_workers=4, pin_memory=True, sampler=sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, labels in data_loader:\n",
    "    imgs=imgs.to(device)\n",
    "    labels=labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.max(), imgs.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Architecture Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Generator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_block(scale_factor, in_channels, out_channels, kernel_size, stride=1, padding=0, last_layer = True):\n",
    "    return (\n",
    "            # nn.ConvTranspose2d(in_channels=in_channels, out_channels=in_channels, kernel_size=4,stride=2,padding=1), # doubles up the shape\n",
    "            nn.Upsample(scale_factor=scale_factor, mode ='bilinear', align_corners = True),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            # nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            # nn.BatchNorm2d(out_channels),\n",
    "            # nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2,inplace=True) if not last_layer else nn.Tanh()\n",
    "    )\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, label_dim, n_hidden_layers, hidden_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=z_dim, out_features=z_dim*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=z_dim*4, out_features=32*32*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "\n",
    "        self.label_projector = nn.Sequential(\n",
    "            nn.Linear(in_features=label_dim, out_features=256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=32*32*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "\n",
    "        self.in_channels = 4 # after the fully connected network it will be converted to 32x32 images\n",
    "        layers = [\n",
    "            *upsample_block(scale_factor=2, in_channels=self.in_channels, out_channels=hidden_channels[0], kernel_size=3, stride=1, padding=1, last_layer=False),\n",
    "        ]\n",
    "        for i in range(n_hidden_layers-1):\n",
    "            if i==n_hidden_layers-2:\n",
    "                # if last upsample block then it will add sigmoid else relu\n",
    "                ub = *upsample_block(scale_factor=2, in_channels=hidden_channels[i], out_channels=hidden_channels[i+1], kernel_size=3, stride=1, padding=1, last_layer=True),\n",
    "            else:\n",
    "                ub = *upsample_block(scale_factor=2, in_channels=hidden_channels[i], out_channels=hidden_channels[i+1], kernel_size=3, stride=1, padding=1, last_layer=False),\n",
    "            for l in ub:\n",
    "                layers.append(l)\n",
    "\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # cat = torch.cat((z, labels), axis=1)\n",
    "\n",
    "        labels = labels.type(torch.float32)\n",
    "        \n",
    "        projected = self.fc(z)\n",
    "\n",
    "        label_projected = self.label_projector(labels)\n",
    "\n",
    "        cat = torch.cat((projected, label_projected), -1)\n",
    "\n",
    "        reshaped = cat.view((-1, self.in_channels, 32, 32))\n",
    "\n",
    "        upsampled = self.upsample(reshaped)\n",
    "\n",
    "        return upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Instance creation and shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dim=64, label_dim=num_classes, n_hidden_layers=3, hidden_channels=[128, 128, 3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn((BATCH,64)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.device, labels.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = generator(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Discriminator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=64, kernel_size=4, stride=2, padding=0, bias=False), # 128\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0, bias=False), # 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, padding=0, bias=False), # 32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=2, stride=2, padding=0, bias=False), # 16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=4, stride=2, padding=0, bias=False), # 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=0, bias=False), # 2\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.label_projector = nn.Sequential(\n",
    "            nn.Linear(in_features=num_classes, out_features=256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features=2*2*128, out_features=256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.LeakyReLU(0.2,inplace=True)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=128, out_features=16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=16, out_features=1),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, labels):\n",
    "        labels = labels.type(torch.float32)\n",
    "\n",
    "        conved = self.critic(images)\n",
    "\n",
    "        flat = conved.view((conved.shape[0], 2*2*128))\n",
    "\n",
    "        features = self.feature_extractor(flat)\n",
    "\n",
    "        label_projected = self.label_projector(labels)\n",
    "\n",
    "        cat = torch.cat((features, label_projected), 1) # concat in axis=1\n",
    "        # added = flat+label_projected\n",
    "        output = self.fc(cat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Instance creation and shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(num_classes=num_classes, img_channels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = discriminator(fake, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loss Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_loss(loss_fn, gen_model, disc_model, images, labels, z_dim):\n",
    "    batch = labels.shape[0]\n",
    "    noise = torch.randn((batch, z_dim)).to(device)\n",
    "\n",
    "    fake_imgs = gen_model(noise, labels)\n",
    "\n",
    "    fake_pred = disc_model(fake_imgs.detach(), labels) # detach the generator while training the discriminator\n",
    "    real_pred = disc_model(images, labels)\n",
    "\n",
    "    fake_target = torch.zeros_like(fake_pred)+0.05\n",
    "    real_target = torch.ones_like(real_pred)*0.95\n",
    "\n",
    "    fake_loss = loss_fn(fake_pred, fake_target)\n",
    "    real_loss = loss_fn(real_pred, real_target)\n",
    "\n",
    "    return (fake_loss + real_loss)/2\n",
    "\n",
    "def gen_loss(loss_fn, gen_model, disc_model, labels, z_dim):\n",
    "    batch = labels.shape[0]\n",
    "    noise = torch.randn((batch, z_dim)).to(device)\n",
    "\n",
    "    fake_images = gen_model(noise, labels)\n",
    "    fake_pred = disc_model(fake_images, labels)\n",
    "\n",
    "    fake_target = torch.ones_like(fake_pred)*0.95\n",
    "    loss = loss_fn(fake_pred, fake_target)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def show(tensor, ch = 3, size = (256,256), n_imgs = 25, epoch = 0, save=False):\n",
    "    data = tensor.detach().cpu().view(-1, ch, *size)\n",
    "    reshaped = make_grid(data[:n_imgs], nrow=5).permute(1,2,0) # from C,H,W to H,W,C\n",
    "    plt.imshow(reshaped)\n",
    "    plt.title(f\"Epoch: {epoch}\")\n",
    "    plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(f\"/kaggle/working/images/fake_{epoch}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training Setup and Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Hyper Parameters and Optimizers Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0003\n",
    "k=2\n",
    "log_step=5\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = nn.BCEWithLogitsLoss()\n",
    "gen_opt = torch.optim.Adam(generator.parameters(), lr = lr, betas=(0.5,0.99))\n",
    "disc_opt = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5,0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Training Loop and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn((25, 64)).to(device)\n",
    "test_labels = F.one_hot(torch.LongTensor([i+1 for i in range(25)]), num_classes=num_classes).to(device)\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"EPOCH: {i} startng...\")\n",
    "    mean_gen_loss = 0\n",
    "    mean_disc_loss = 0\n",
    "    batch = 0\n",
    "    for imgs, labels in tqdm(data_loader):\n",
    "        b = labels.shape[0]\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        for ik in range(k):\n",
    "            disc_opt.zero_grad()\n",
    "            disc_loss_ = disc_loss(loss_fn=base_loss, gen_model=generator, disc_model=discriminator, images=imgs, labels=labels, z_dim=64)\n",
    "            mean_disc_loss += disc_loss_.item()*b/k\n",
    "            disc_loss_.backward()\n",
    "            disc_opt.step()\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        gen_loss_ = gen_loss(loss_fn=base_loss, gen_model=generator, disc_model=discriminator, labels=labels, z_dim=64)\n",
    "        mean_gen_loss += gen_loss_.item()*b\n",
    "        gen_loss_.backward()\n",
    "        gen_opt.step()\n",
    "        batch+=b\n",
    "\n",
    "\n",
    "        # del gen_loss_, disc_loss_\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    print(f\"Mean Gen Loss: {mean_gen_loss}, Mean Disc Loss: {mean_disc_loss}\")\n",
    "    if i%log_step==0:\n",
    "        with torch.no_grad():\n",
    "            fake = (generator(z, test_labels)+1)/2\n",
    "            # fake = generator(z, test_labels)\n",
    "        show(fake, epoch=i, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = glob.glob('results/images/*.png')\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/images\\\\fake_0.png',\n",
       " 'results/images\\\\fake_5.png',\n",
       " 'results/images\\\\fake_10.png',\n",
       " 'results/images\\\\fake_15.png',\n",
       " 'results/images\\\\fake_20.png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "imgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [Image.open(i) for i in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].save('generation progress.gif', save_all=True, append_images = images[1:], duration=200, loop=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2227015,
     "sourceId": 3724057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
